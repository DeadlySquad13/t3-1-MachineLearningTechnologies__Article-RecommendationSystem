{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f37888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dd7454a",
   "metadata": {},
   "source": [
    "# Цель работы\n",
    "Необходимо предложить пользователям Яндекс.Карт соответствующие их вкусу\n",
    "кафе, бары и рестораны в неродном городе: москвичам – в Санкт-Петербурге, а\n",
    "петербуржцам – в Москве. В качестве данных предоставлена анонимизированная\n",
    "информация о реальных отзывах и оценках, оставляемых пользователями\n",
    "Яндекс.Карт на заведения общепита Москвы и Санкт-Петербурга, и различная\n",
    "информация о самих заведениях.\n",
    "\n",
    "В частности, каждый отзыв содержит множество\n",
    "аспектов (упомянутые в отзыве блюда, особенности и т. п.), извлеченных из\n",
    "отзыва с помощью NLP-алгоритма. Для заданного множества москвичей и\n",
    "петербуржцев нужно предсказать, какие заведения в неродном городе они\n",
    "посетят, оставив при этом положительный отзыв с оценкой 4 или 5.\n",
    "\n",
    "Так как данных очень много, перед тем как приступить к анализу, проведем обзор данных и, возможно, потребуется их предобработка, чтобы датасет стал более удобным и пригодным к проведению исследования.\n",
    "\n",
    "Таким образом исследование пройдет в 3 этапа:\n",
    "- обзор данных,\n",
    "- предобработка данных,\n",
    "- анализ отзывов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874ae6c",
   "metadata": {},
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f81aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основные библиотеки.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Визуализция.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Для матрицы взаимодействий.\n",
    "from scipy import sparse\n",
    "\n",
    "# Отрисовка статуса выполнения.\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Типизация.\n",
    "from typing import List\n",
    "\n",
    "# Настройки отрисовки графиков.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aaad83",
   "metadata": {},
   "source": [
    "# Обзор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90675575",
   "metadata": {},
   "source": [
    "## Считываем данные из .csv\n",
    "Загрузим файлы датасета в помощью библиотеки Pandas.\n",
    "\n",
    "Не смотря на то, что файлы имеют расширение txt они представляют собой данные\n",
    "в формате [CSV](https://ru.wikipedia.org/wiki/CSV). Часто в файлах такого\n",
    "формата в качестве разделителей используются символы \",\", \";\" или табуляция.\n",
    "Поэтому вызывая метод read_csv всегда стоит явно указывать разделитель данных\n",
    "с помощью параметра sep. Чтобы узнать какой разделитель используется в файле\n",
    "его рекомендуется предварительно посмотреть в любом текстовом редакторе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086491ef",
   "metadata": {},
   "source": [
    "В первую очередь нам понадобятся данные по **пользователям**, **организациям** и сами **отзывы**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f523b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('data/users.csv', sep=',')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de66fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = pd.read_csv('data/organisations.csv', sep=',')\n",
    "orgs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем датасет по частям, затем соединяем в один. Позволяет загружать большие\n",
    "#   (500 мб и более) датасеты.\n",
    "#   К сожалению, даже при low_memory=True иногда падает кернел, несмотря на то\n",
    "#   что внутри используется схожий механизм.\n",
    "# - Количество рядов, содеражихся в одной части датасета.\n",
    "chunksize = 1000\n",
    "reviews = pd.concat(pd.read_csv('data/reviews.csv', chunksize=chunksize))\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0779a31",
   "metadata": {},
   "source": [
    "Некоторые данные (такие как рубрики и признаки), представлены строками значений. Преобразуем их в списки чисел. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ff309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переводит строку вида '1 2 3 4' в список [1, 2, 3, 4].\n",
    "to_int_list = lambda values: [int(value) for value in str(values).split(' ')]\n",
    "\n",
    "def apply_to_columns(df: pd.DataFrame, columns: List[str], func=to_int_list):\n",
    "    \"\"\"Apply function to the specified columns of the dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    for column in columns:\n",
    "        df.loc[~df[column].isnull(), column] = df.loc[\n",
    "            ~df[column].isnull(), column\n",
    "        ].apply(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18173f70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create lists\n",
    "columns = ['rubrics_id', 'features_id']\n",
    "apply_to_columns(orgs, columns)\n",
    "\n",
    "orgs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5b4c1",
   "metadata": {},
   "source": [
    "Чтобы не делать __join__ каждый раз, когда нам потребуется узнать, из какого города организация или пользователь, сразу добавим эту информацию в отзывы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join по user_id.\n",
    "reviews = reviews.merge(users, on='user_id')\n",
    "reviews = reviews.rename({'city': 'user_city'}, axis=1)\n",
    "\n",
    "# Join по org_id.\n",
    "reviews = reviews.merge(orgs[['org_id', 'city']], on='org_id')\n",
    "reviews = reviews.rename({'city': 'org_city'}, axis=1)\n",
    "\n",
    "# В колонке aspects тоже находятся записи вида '1 2 3', приведём их к числовому\n",
    "#   списку.\n",
    "columns = ['aspects']\n",
    "apply_to_columns(reviews, columns)\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab4d81",
   "metadata": {},
   "source": [
    "Отлично, теперь с отзывами будет удобно работать. \n",
    "\n",
    "Посмотрим на распределение новых отзывов по дням, чтобы понять, как лучше организовать валидацию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=reviews, x='ts', height=8)\n",
    "plt.title('Распределение отзывов по дням')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88036f5e",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2616357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_df(df, suffixes=['_x', '_y'], inplace=True):\n",
    "    '''\n",
    "    clear_df(df, suffixes=['_x', '_y'], inplace=True)\n",
    "        Удаляет из входного df все колонки, оканчивающиеся на заданные суффиксы. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "        \n",
    "        suffixies : Iterable, default=['_x', '_y']\n",
    "            Суффиксы колонок, подлежащих удалению\n",
    "            \n",
    "        inplace : bool, default=True\n",
    "            Нужно ли удалить колонки \"на месте\" или же создать копию DataFrame.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame (optional)\n",
    "            df с удалёнными колонками\n",
    "    '''\n",
    "    \n",
    "    def bad_suffix(column):\n",
    "        nonlocal suffixes\n",
    "        return any(column.endswith(suffix) for suffix in suffixes)\n",
    "        \n",
    "    columns_to_drop = [col for col in df.columns if bad_suffix(col)]\n",
    "    return df.drop(columns_to_drop, axis=1, inplace=inplace)\n",
    "\n",
    "\n",
    "def extract_unique(reviews, column): \n",
    "    '''\n",
    "    extract_unique(reviews, column)\n",
    "        Извлекает уникальные значения из колонки в DataFrame.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        reviews : pandas.DataFrame\n",
    "            pandas.DataFrame, из которого будут извлечены значения.\n",
    "        \n",
    "        column : str\n",
    "            Имя колонки в <reviews>.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            Содержит одну именованную колонку с уникальными значениями. \n",
    "    '''\n",
    "    \n",
    "    unique = reviews[column].unique()\n",
    "    return pd.DataFrame({column: unique})\n",
    "\n",
    "\n",
    "def count_unique(reviews, column):\n",
    "    '''\n",
    "    count_unique(reviews, column)\n",
    "        Извлекает и подсчитывает уникальные значения из колонки в DataFrame.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        reviews : pandas.DataFrame\n",
    "            pandas.DataFrame, из которого будут извлечены значения.\n",
    "        \n",
    "        column : str\n",
    "            Имя колонки в <reviews>.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            Содержит две колонки: с уникальными значениями и счётчиком встреченных. \n",
    "    '''\n",
    "    \n",
    "    return reviews[column].value_counts().reset_index(name='count').rename({'index': column}, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def filter_reviews(reviews, users=None, orgs=None): \n",
    "    '''\n",
    "    filter_reviews(reviews, users=None, orgs=None)\n",
    "    Оставляет в выборке только отзывы, оставленные заданными пользователями на заданные организации. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        users: pandas.DataFrame, default=None\n",
    "            DataFrame, содержащий колонку <user_id>.\n",
    "            Если None, то фильтрация не происходит. \n",
    "            \n",
    "        orgs: pandas.DataFrame, default=None\n",
    "            DataFrame, содержащий колонку <org_id>.\n",
    "            Если None, то фильтрация не происходит. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        pandas.DataFrame\n",
    "            Отфильтрованная выборка отзывов. \n",
    "\n",
    "    '''\n",
    "    if users is not None: \n",
    "        reviews = reviews.merge(users, on='user_id', how='inner')\n",
    "        clear_df(reviews)\n",
    "        \n",
    "    if orgs is not None:\n",
    "        reviews = reviews.merge(orgs, on='org_id', how='inner')\n",
    "        clear_df(reviews)\n",
    "        \n",
    "    return reviews\n",
    "\n",
    "\n",
    "def train_test_split(reviews, ts_start, ts_end=None):\n",
    "    '''\n",
    "    train_test_split(reviews, ts_start, ts_end=None)\n",
    "        Разделяет выборку отзывов на две части: обучающую и тестовую. \n",
    "        В тестовую выборку попадают только отзывы с user_id и org_id, встречающимися в обучающей выборке.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reviews : pandas.DataFrame \n",
    "            Отзывы из reviews.csv с обязательными полями:\n",
    "                <rating>, <ts>, <user_id>, <user_city>, <org_id>, <org_city>.\n",
    "\n",
    "        ts_start : int\n",
    "            Первый день отзывов из тестовой выборки (включительно).\n",
    "\n",
    "        ts_end : int, default=None\n",
    "            Последний день отзывов из обучающей выборки (включительно)\n",
    "            Если параметр равен None, то ts_end == reviews['ts'].max(). \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        splitting : tuple\n",
    "            Кортеж из двух pandas.DataFrame такой же структуры, как и reviews:\n",
    "            в первом отзывы, попавшие в обучающую выборку, во втором - в тестовую.\n",
    "    '''\n",
    "    \n",
    "    if not ts_end:\n",
    "        ts_end = reviews['ts'].max()\n",
    "    \n",
    "    \n",
    "    reviews_train = reviews[(reviews['ts'] < ts_start) | (reviews['ts'] > ts_end)]\n",
    "    reviews_test = reviews[(ts_start <= reviews['ts']) & (reviews['ts'] <= ts_end)]\n",
    "    \n",
    "    # 1. Выбираем только отзывы на понравившиеся места у путешественников\n",
    "    reviews_test = reviews_test[reviews_test['rating'] >= 4.0]\n",
    "    user_and_org_from_different_cities = reviews_test['org_city'] != reviews_test['user_city']\n",
    "    reviews_test = reviews_test[user_and_org_from_different_cities]\n",
    "    \n",
    "    # 2. Оставляем в тесте только тех пользователей и организации, которые встречались в трейне\n",
    "    train_orgs = extract_unique(reviews_train, 'org_id')\n",
    "    train_users = extract_unique(reviews_train, 'user_id')\n",
    "    \n",
    "    reviews_test = filter_reviews(reviews_test, orgs=train_orgs)\n",
    "\n",
    "    return reviews_train, reviews_test\n",
    "\n",
    "\n",
    "def process_reviews(reviews):\n",
    "    '''\n",
    "    process_reviews(reviews)\n",
    "        Извлекает из набора отзывов тестовых пользователей и таргет. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        reviews : pandas.DataFrame\n",
    "            DataFrame с отзывами, содержащий колонки <user_id> и <org_id>\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X : pandas.DataFrame\n",
    "            DataFrame такой же структуры, как и в test_users.csv\n",
    "            \n",
    "        y : pandas.DataFrame\n",
    "            DataFrame с колонками <user_id> и <target>. \n",
    "            В <target> содержится список org_id, посещённых пользователем. \n",
    "    '''\n",
    "    \n",
    "    y = reviews.groupby('user_id')['org_id'].apply(list).reset_index(name='target')\n",
    "    X = pd.DataFrame(y['user_id'])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b767b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['ts'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e39019",
   "metadata": {},
   "source": [
    "Всего в выборку попали отызывы за **1216** дней. \n",
    "\n",
    "Отложим в тестовую выборку отзывы за последние **100** дней."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71211d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews, test_reviews = train_test_split(reviews, 1116)\n",
    "X_test, y_test = process_reviews(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ce241",
   "metadata": {},
   "source": [
    "Посмотрим, сколько всего уникальных пользователей попало в эту тестовую выборку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4c71a",
   "metadata": {},
   "source": [
    "# Метрика"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff1c684",
   "metadata": {},
   "source": [
    "Метрика принимает на вход два DataFrame, имеющих такую же структуру, как и **y_test**.\n",
    "Вычисляется Precision, в контексте рекомендательных систем формула выглядит\n",
    "следующим образом: \n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\qquad P = \\frac{\\textrm{Количество релевантных рекомендаций}}{\\textrm{Количество рекомендаций}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Далее берётся Average Precision:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\textrm{AP@N} = \\frac{1}{m}\\sum_{k=1}^N \\textrm{($P(k)$ если $k_{ая}$ рекомендация была релевантна)} = \\frac{1}{m}\\sum_{k=1}^N P(k)\\cdot rel(k),\n",
    "\\end{align*}\n",
    "$$\n",
    "где m - количество рекомендаций, N - количество рекомендаций, которые мы\n",
    "берём в расчёт $ N \\leq m $\n",
    "\n",
    "В конечном итоге, нас интересует Mean Average Precision - среднее Average\n",
    "Precision по пользователям:\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\textrm{MAP@N} = \\frac{1}{|U|}\\sum_{u=1}^|U|(\\textrm{AP@N})_u = \\frac{1}{|U|} \\sum_{u=1}^|U| \\frac{1}{m}\\sum_{k=1}^N P_u(k)\\cdot rel_u(k).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c81286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision_at_n(size=20):\n",
    "    '''\n",
    "    mean_average_precision_at_n(size=20)\n",
    "        Создаёт метрику под <size> сделанных предсказаний.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        size : int, default=20\n",
    "            Размер рекомендованной выборки для каждого пользователя\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        func(pd.DataFrame, pd.DataFrame) -> float\n",
    "            Функция, вычисляющая mean_average_precision_at_n.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    assert size >= 1, \"Size must be greater than zero!\"\n",
    "    \n",
    "    def metric(y_true, predictions, size=size):\n",
    "        '''\n",
    "        metric(y_true, predictions, size=size)\n",
    "            Метрика mean_average_precision_at_n для двух перемешанных наборов <y_true> и <y_pred>.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            y_true : pd.DataFrame\n",
    "                DataFrame с колонками <user_id> и <target>. \n",
    "                В <target> содержится список настоящих org_id, посещённых пользователем. \n",
    "                \n",
    "            predictions : pd.DataFrame\n",
    "                DataFrame с колонками <user_id> и <target>. \n",
    "                В <target> содержится список рекомендованных для пользователя org_id.\n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            float \n",
    "                Значение метрики.\n",
    "        '''\n",
    "        \n",
    "        y_true = y_true.rename({'target': 'y_true'}, axis='columns')\n",
    "        predictions = predictions.rename({'target': 'predictions'}, axis='columns')\n",
    "        \n",
    "        merged = y_true.merge(predictions, left_on='user_id', right_on='user_id')\n",
    "    \n",
    "        def average_precision_for_user(x: pd.DataFrame, size=size) -> float:\n",
    "            '''\n",
    "            average_precision_for_user(x: pd.DataFrame, size=size) -> float\n",
    "            Средняя точность для пользователя.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            x : pd.DataFrame\n",
    "                DataFrame с колонками <user_id>, <y_true>, <predictions>.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            float\n",
    "                Значение от 0 до 1 - средняя точность для пользователя.\n",
    "            '''\n",
    "            y_true = x[1][1]\n",
    "            predictions = x[1][2]\n",
    "            \n",
    "            weight = 0\n",
    "            \n",
    "            inner_weights = [0]\n",
    "            for n, item in enumerate(predictions):\n",
    "                inner_weight = inner_weights[-1] + (1 if item in y_true else 0)\n",
    "                inner_weights.append(inner_weight)\n",
    "            \n",
    "            for n, item in enumerate(predictions):                \n",
    "                if item in y_true:\n",
    "                    weight += inner_weights[n + 1] / (n + 1)\n",
    "                    \n",
    "            return weight / min(len(y_true), size)\n",
    "\n",
    "        return np.mean([average_precision_for_user(user_row) for user_row in merged.iterrows()])\n",
    "    \n",
    "        \n",
    "    return metric\n",
    "\n",
    "\n",
    "def print_score(score):\n",
    "    print(f\"Score: {score:.6f}\")\n",
    "    \n",
    "    \n",
    "N = 20\n",
    "mapN = mean_average_precision_at_n(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43411422",
   "metadata": {},
   "source": [
    "# Подходы без машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9668cba5",
   "metadata": {},
   "source": [
    "## Случайные N мест"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c7edd",
   "metadata": {},
   "source": [
    "Попробуем предлагать пользователям случайные места из другого города. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02d232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spb_orgs = orgs[orgs['city'] == 'spb']['org_id']\n",
    "msk_orgs = orgs[orgs['city'] == 'msk']['org_id']\n",
    "\n",
    "test_users_with_locations = X_test.merge(users, on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834744b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(1337)\n",
    "choose = lambda x: np.random.choice(spb_orgs, N) if x['user_id'] == 'msk' else np.random.choice(msk_orgs, N)\n",
    "target = test_users_with_locations.apply(choose, axis=1)\n",
    "\n",
    "predictions = X_test.copy()\n",
    "predictions['target'] = target\n",
    "\n",
    "print_score(mapN(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6199951",
   "metadata": {},
   "source": [
    "## N самых популярных мест"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc0eb4d",
   "metadata": {},
   "source": [
    "Предыдущий подход, очевидно, не очень удачно предсказывает, какие места посетит пользователей. \n",
    "\n",
    "Попробуем улучшить стратегию: будем предлагать пользователям самые популярные места, то есть те, на которые оставлено больше всего отзывов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e42340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_orgs = train_reviews[(train_reviews['rating'] >= 4) & (train_reviews['org_city'] == 'msk')]['org_id']\n",
    "msk_orgs = msk_orgs.value_counts().index[:N].to_list()\n",
    "\n",
    "spb_orgs = train_reviews[(train_reviews['rating'] >= 4) & (train_reviews['org_city'] == 'spb')]['org_id']\n",
    "spb_orgs = spb_orgs.value_counts().index[:N].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "choose = lambda x: spb_orgs if x['user_id'] == 'msk' else msk_orgs\n",
    "target = test_users_with_locations.apply(choose, axis=1)\n",
    "\n",
    "predictions = X_test.copy()\n",
    "predictions['target'] = target\n",
    "\n",
    "print_score(mapN(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c29a8",
   "metadata": {},
   "source": [
    "Отлично, метрика немного улучшилась. Но стоит попробовать доработать эту тактику. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa30699",
   "metadata": {},
   "source": [
    "## N самых популярных мест среди туристов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourist_reviews = train_reviews[train_reviews['rating'] >= 4.0]\n",
    "\n",
    "# набор отзывов только от туристов\n",
    "tourist_reviews = tourist_reviews[tourist_reviews['user_city'] != tourist_reviews['org_city']]\n",
    "\n",
    "# выбираем самые популярные места среди туристов из Москвы и Питера\n",
    "msk_orgs = tourist_reviews[tourist_reviews['org_city'] == 'msk']['org_id']\n",
    "msk_orgs = msk_orgs.value_counts().index[:N].to_list()\n",
    "\n",
    "spb_orgs = tourist_reviews[tourist_reviews['org_city'] == 'spb']['org_id']\n",
    "spb_orgs = spb_orgs.value_counts().index[:N].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef317e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "choose = lambda x: spb_orgs if x['user_id'] == 'msk' else msk_orgs\n",
    "target = test_users_with_locations.apply(choose, axis=1)\n",
    "\n",
    "predictions = X_test.copy()\n",
    "predictions['target'] = target\n",
    "\n",
    "print_score(mapN(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d960378",
   "metadata": {},
   "source": [
    "Метрика улучшилась ещё немного."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcbf38a",
   "metadata": {},
   "source": [
    "## N / rubrics_count самых популярных мест из каждой рубрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_by_rubrics(reviews, N):\n",
    "    '''\n",
    "    extract_top_by_rubrics(reviews, N)\n",
    "        Набирает самые популярные организации по рубрикам, сохраняя распределение.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        reviews : pd.DataFrame\n",
    "            Отзывы пользователей для рекомендации.\n",
    "            \n",
    "        N : int\n",
    "            Число рекомендаций.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        orgs_list : list\n",
    "            Список отобранных организаций.\n",
    "    '''\n",
    "    \n",
    "    # Извлечение популярных рубрик.\n",
    "    reviews = reviews.merge(orgs, on='org_id')[['org_id', 'rubrics_id']]\n",
    "    \n",
    "    rubrics = reviews.explode('rubrics_id').groupby('rubrics_id').size()\n",
    "    rubrics = (rubrics / rubrics.sum() * N).apply(round).sort_values(ascending=False)\n",
    "\n",
    "    # Вывод списка рубрик по убыванию популярности.\n",
    "    print(\n",
    "        pd.read_csv('data/rubrics.csv', sep=',')\n",
    "        .merge(rubrics.reset_index(), left_index=True, right_on='rubrics_id')\n",
    "        .sort_values(by=0, ascending=False)[['rubric_id', 0]]\n",
    "    )\n",
    "    \n",
    "    # извлечение популярных организаций\n",
    "    train_orgs = reviews.groupby('org_id').size().reset_index(name='count').merge(orgs, on='org_id')\n",
    "    train_orgs = train_orgs[['org_id', 'count', 'rubrics_id']]\n",
    "\n",
    "    most_popular_rubric = lambda rubrics_id: max(rubrics_id, key=lambda rubric_id: rubrics[rubric_id])\n",
    "    train_orgs['rubrics_id'] = train_orgs['rubrics_id'].apply(most_popular_rubric)\n",
    "    \n",
    "    orgs_by_rubrics = train_orgs.sort_values(by='count', ascending=False).groupby('rubrics_id')['org_id'].apply(list)\n",
    "    \n",
    "    # соберём самые популярные организации в рубриках в один список\n",
    "    \n",
    "    orgs_list = []\n",
    "\n",
    "    for rubric_id, count in zip(rubrics.index, rubrics):\n",
    "        if rubric_id not in orgs_by_rubrics:\n",
    "            continue \n",
    "\n",
    "        orgs_list.extend(orgs_by_rubrics[rubric_id][:count])\n",
    "    \n",
    "    return orgs_list\n",
    "\n",
    "\n",
    "msk_orgs = extract_top_by_rubrics(tourist_reviews[tourist_reviews['org_city'] == 'msk'], N)\n",
    "spb_orgs = extract_top_by_rubrics(tourist_reviews[tourist_reviews['org_city'] == 'spb'], N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b601803",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "choose = lambda x: spb_orgs if x['user_id'] == 'msk' else msk_orgs\n",
    "target = test_users_with_locations.apply(choose, axis=1)\n",
    "\n",
    "predictions = X_test.copy()\n",
    "predictions['target'] = target\n",
    "\n",
    "print_score(mapN(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd4dfe",
   "metadata": {},
   "source": [
    "# ML методы. Коллаборативная фильтрация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a635430",
   "metadata": {},
   "source": [
    "## Memory-based\n",
    "\n",
    "Для этой группы методов требуется явное построение матрицы __пользователь-организация__ (__interaction matrix__), где на пересечении $i$-ой строки и $j$-ого столбца будет рейтинг, который $i$-ый пользователь выставил $j$-ой организации или же пропуск, если рейтинг не был установлен. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4602e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_reviews(reviews, min_user_reviews=5, min_org_reviews=13):\n",
    "    '''\n",
    "    reduce_reviews(reviews, min_user_reviews=5, min_org_reviews=13)\n",
    "        Убирает из выборки пользователей и организации, у которых менее <min_reviews> отзывов в родном городе. \n",
    "        Оставляет только отзывы туристов. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        reviews : pandas.DataFrame \n",
    "            Выборка отзывов с обязательными полями:\n",
    "                <user_id>, <user_city>.\n",
    "        \n",
    "        min_user_reviews : int, default=5\n",
    "            Минимальное количество отзывов у пользователя, необходимое для включения в выборку.\n",
    "            \n",
    "        min_org_reviews : int, default=13\n",
    "            Минимальное количество отзывов у организации, необходимое для включения в выборку.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        splitting : tuple\n",
    "            Кортеж из двух наборов.\n",
    "            Каждый набор содержит 2 pandas.DataFrame:\n",
    "                1. Урезанная выборка отзывов\n",
    "                2. Набор уникальных организаций\n",
    "                \n",
    "            Первый набор содержит DataFrame-ы, относящиеся к отзывам, оставленным в родном городе, а второй -\n",
    "            к отзывам, оставленным в чужом городе. ё\n",
    "            \n",
    "        users : pd.DataFrame\n",
    "            Набор уникальных пользователей в выборке\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    inner_reviews = reviews[reviews['user_city'] == reviews['org_city']]\n",
    "    outer_reviews = reviews[reviews['user_city'] != reviews['org_city']]\n",
    "\n",
    "    # оставляем только отзывы туристов на родной город \n",
    "    tourist_users = extract_unique(outer_reviews, 'user_id')\n",
    "    inner_reviews = filter_reviews(inner_reviews, users=tourist_users)\n",
    "    \n",
    "    # выбираем только тех пользователей и организации, у которых есть <min_reviews> отзывов\n",
    "    top_users = count_unique(inner_reviews, 'user_id')\n",
    "    top_users = top_users[top_users['count'] >= min_user_reviews]\n",
    "        \n",
    "    top_orgs = count_unique(inner_reviews, 'org_id')\n",
    "    top_orgs = top_orgs[top_orgs['count'] >= min_org_reviews]\n",
    "        \n",
    "    inner_reviews = filter_reviews(inner_reviews, users=top_users, orgs=top_orgs)\n",
    "    outer_reviews = filter_reviews(outer_reviews, users=top_users)\n",
    "    pd.pivot_table(outer_reviews, index='user_id', columns='org_id', aggfunc=len, fill_value=0)\n",
    "    display(outer_reviews)\n",
    "    \n",
    "    # combine reviews\n",
    "    reviews = pd.concat([inner_reviews, outer_reviews])\n",
    "    users = extract_unique(reviews, 'user_id')\n",
    "    orgs = extract_unique(reviews, 'org_id')\n",
    "    \n",
    "    \n",
    "    return (\n",
    "        (\n",
    "            inner_reviews,\n",
    "            extract_unique(inner_reviews, 'org_id')\n",
    "        ),\n",
    "        (\n",
    "            outer_reviews,\n",
    "            extract_unique(outer_reviews, 'org_id')\n",
    "        ),\n",
    "        extract_unique(inner_reviews, 'user_id')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8df60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mappings(df, column):\n",
    "    '''\n",
    "    create_mappings(df, column)\n",
    "        Создаёт маппинг между оригинальными ключами словаря и новыми порядковыми.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            DataFrame с данными.\n",
    "            \n",
    "        column : str\n",
    "            Название колонки, содержащей нужны ключи. \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        code_to_idx : dict\n",
    "            Словарь с маппингом: \"оригинальный ключ\" -> \"новый ключ\".\n",
    "        \n",
    "        idx_to_code : dict\n",
    "            Словарь с маппингом: \"новый ключ\" -> \"оригинальный ключ\".\n",
    "    '''\n",
    "    \n",
    "    code_to_idx = {}\n",
    "    idx_to_code = {}\n",
    "    \n",
    "    for idx, code in enumerate(df[column].to_list()):\n",
    "        code_to_idx[code] = idx\n",
    "        idx_to_code[idx] = code\n",
    "        \n",
    "    return code_to_idx, idx_to_code\n",
    "\n",
    "\n",
    "def map_ids(row, mapping):\n",
    "    '''\n",
    "    Вспомогательная функция\n",
    "    '''\n",
    "    \n",
    "    return mapping[row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a61e901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def interaction_matrix_1(reviews, test_users, min_user_reviews=15, min_org_reviews=1): \n",
    "    '''\n",
    "    interaction_matrix(reviews, test_users, min_user_reviews=5, min_org_reviews=12)\n",
    "        Создаёт блочную матрицу взаимодействий (вид матрицы описан в Returns)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        reviews : pd.DataFrame\n",
    "            Отзывы пользователей для матрицы взаимодействий.\n",
    "            \n",
    "        test_users : pd.DataFrame\n",
    "            Пользователи, для которых будет выполнятся предсказание. \n",
    "        \n",
    "        min_user_reviews : int, default=5\n",
    "            Минимальное число отзывов от пользователя, необходимое для включения его в матрицу.\n",
    "        \n",
    "        min_org_reviews : int, default=12\n",
    "            Минимальное число отзывов на организацию, необходимое для включения её в матрицу.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        InteractionMatrix : scipy.sparse.csr_matrix\n",
    "            Матрица, содержащая рейтинги, выставленные пользователями.\n",
    "    '''\n",
    "    user_col = 'user_id'\n",
    "    item_col = 'org_id'\n",
    "    interact_col = 'rating'\n",
    "\n",
    "\t# Extract unique ids.\n",
    "    users = np.array(list(\n",
    "                        np.sort(reviews[user_col].unique())\n",
    "                     ))  # Get our unique users\n",
    "\n",
    "    items = np.array(list(\n",
    "                        reviews[item_col].unique()\n",
    "                     ))  # Get our unique items that were interacted with\n",
    "\n",
    "    interactions = list(\n",
    "        reviews[interact_col])  # All of our interactions\n",
    "\n",
    "    # Re-code user and item index for sparse matrix.\n",
    "    # Index starting from 0\n",
    "    rows = reviews[user_col].astype(pd.CategoricalDtype(\n",
    "        categories=users)).cat.codes\n",
    "\n",
    "    cols = reviews[item_col].astype(pd.CategoricalDtype(\n",
    "        categories=items)).cat.codes\n",
    "\n",
    "    # Construct sparse matrix.\n",
    "    # Get the associated column indices\n",
    "    interaction_sparse = sparse.csr_matrix(\n",
    "        (interactions, (rows, cols)), shape=(len(users), len(items))\n",
    "    )\n",
    "\n",
    "    return interaction_sparse, users, items\n",
    "\n",
    "msk_interactions, users, items = interaction_matrix_1(\n",
    "    train_reviews[train_reviews['user_city'] == 'msk'],\n",
    "    test_users_with_locations[test_users_with_locations['city'] == 'msk'],\n",
    ")\n",
    "\n",
    "display(users, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95959877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interaction_matrix(reviews, test_users, min_user_reviews=15, min_org_reviews=1): \n",
    "    '''\n",
    "    interaction_matrix(reviews, test_users, min_user_reviews=5, min_org_reviews=12)\n",
    "        Создаёт блочную матрицу взаимодействий (вид матрицы описан в Returns)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        reviews : pd.DataFrame\n",
    "            Отзывы пользователей для матрицы взаимодействий.\n",
    "            \n",
    "        test_users : pd.DataFrame\n",
    "            Пользователи, для которых будет выполнятся предсказание. \n",
    "        \n",
    "        min_user_reviews : int, default=5\n",
    "            Минимальное число отзывов от пользователя, необходимое для включения его в матрицу.\n",
    "        \n",
    "        min_org_reviews : int, default=12\n",
    "            Минимальное число отзывов на организацию, необходимое для включения её в матрицу.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        InteractionMatrix : scipy.sparse.csr_matrix\n",
    "            Матрица, содержащая рейтинги, выставленные пользователями.\n",
    "            Она блочная и имеет такой вид:\n",
    "                 ---------------------------------------------------\n",
    "                | TRAIN USERS, INNER ORGS | TRAIN USERS, OUTER ORGS |\n",
    "                |                         |                         |\n",
    "                 ---------------------------------------------------\n",
    "                |  TEST USERS, INNER ORGS |  TEST USERS, OUTER ORGS |\n",
    "                |                         |                         |\n",
    "                 ---------------------------------------------------\n",
    "\n",
    "        splitting : tuple\n",
    "            Кортеж, содержащий два целых числа: \n",
    "                1. Число пользователей в обучающей выборке \n",
    "                2. Число организаций в домашнем регионе\n",
    "\n",
    "        splitting: tuple\n",
    "            Кортеж, содержащий два кортежа из двух словарей:\n",
    "                1. (idx_to_uid, uid_to_idx) - содержит маппинг индекса к user_id\n",
    "                2. (idx_to_oid, oid_to_idx) - содержит маппинг индекса к org_id\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    info = reduce_reviews(train_reviews, min_user_reviews, min_org_reviews)\n",
    "    (inner_reviews, inner_orgs), (outer_reviews, outer_orgs), train_users = info\n",
    "    \n",
    "    # Удалим из обучающей выборки пользователей, которые есть в тестовой.\n",
    "    test_users = test_users[['user_id']]\n",
    "    \n",
    "    train_users = (\n",
    "        pd.merge(train_users, test_users, indicator=True, how='outer')\n",
    "        .query('_merge==\"left_only\"')\n",
    "        .drop('_merge', axis=1)\n",
    "    )\n",
    "    \n",
    "    inner_reviews = filter_reviews(inner_reviews, train_users)\n",
    "    outer_reviews = filter_reviews(outer_reviews, train_users)\n",
    "    \n",
    "    # Оставляем отзывы, оставленные тестовыми пользователями.\n",
    "    test_reviews = filter_reviews(reviews, test_users, pd.concat([inner_orgs, outer_orgs]))\n",
    "    \n",
    "    # Получаем полный набор маппингов.\n",
    "    all_users = pd.concat([train_users, test_users])\n",
    "    all_orgs = pd.concat([inner_orgs, outer_orgs])\n",
    "    \n",
    "    uid_to_idx, idx_to_uid = create_mappings(all_users, 'user_id')\n",
    "    oid_to_idx, idx_to_oid = create_mappings(all_orgs, 'org_id')\n",
    "    \n",
    "    # Собираем матрицу взаимодействий.\n",
    "    reviews = pd.concat([inner_reviews, outer_reviews, test_reviews])    \n",
    "        \n",
    "    I = reviews['user_id'].apply(map_ids, args=[uid_to_idx]).values\n",
    "    J = reviews['org_id'].apply(map_ids, args=[oid_to_idx]).values\n",
    "    values = reviews['rating']\n",
    "        \n",
    "    interactions = sparse.coo_matrix(\n",
    "        (values, (I, J)), \n",
    "        shape=(len(all_users), len(all_orgs)), \n",
    "        dtype=np.float64\n",
    "    ).tocsr()\n",
    "    \n",
    "    \n",
    "    return (\n",
    "        interactions, \n",
    "        (len(train_users), len(inner_orgs)), \n",
    "        (\n",
    "            (idx_to_uid, uid_to_idx),\n",
    "            (idx_to_oid, oid_to_idx)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# msk_interactions = interaction_matrix(\n",
    "    # train_reviews[train_reviews['user_city'] == 'msk'],\n",
    "    # test_users_with_locations[test_users_with_locations['city'] == 'msk'],\n",
    "# )\n",
    "\n",
    "# spb_interactions = interaction_matrix(\n",
    "    # train_reviews[train_reviews['user_city'] == 'spb'],\n",
    "    # test_users_with_locations[test_users_with_locations['city'] == 'spb'],\n",
    "# )       \n",
    "        \n",
    "# test_msk_users = test_users_with_locations[test_users_with_locations['city'] == 'msk']\n",
    "# test_spb_users = test_users_with_locations[test_users_with_locations['city'] == 'spb']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b919c799",
   "metadata": {},
   "source": [
    "## Alternating Least Squares\n",
    "Метод не столь точный как градиентный спуск, но достаточно эффективный:\n",
    "главным достоинством является скорость, даже на больших данных показывает\n",
    "хорошие результаты [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74919a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "def make_predictions(interactions, X_test, N):\n",
    "    '''\n",
    "    make_predictions(interactions, X_test, N)\n",
    "        Делает рекомендации для пользователей из <X_test> на основе матрицы взаимодействий. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        interactions : scipy.sparse.csr_matrix\n",
    "            Разреженная матрица взаимодействий.\n",
    "            \n",
    "        X_test : pd.DataFrame\n",
    "            Набор тестовых пользователей, для которых нужно сделать рекомендации. \n",
    "        \n",
    "        N : int\n",
    "            Число рекомендаций для каждого пользователя. \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        predictions : pd.DataFrame\n",
    "            DataFrame с колонками <user_id> и <target>. \n",
    "            В <target> содержится список рекомендованных для пользователя org_id.\n",
    "    '''\n",
    "    \n",
    "    # Создаём DataFrame со всеми пользователями, но с незаполненными\n",
    "    #   рекомендациями.\n",
    "    predictions = X_test[['user_id']].copy()\n",
    "    predictions['target'] = pd.Series(dtype=object)\n",
    "    predictions = predictions.set_index('user_id')\n",
    "    \n",
    "    # interactions, (train_users_len, inner_orgs_len), mappings = interactions\n",
    "    # (idx_to_uid, uid_to_idx), (idx_to_oid, oid_to_idx) = mappings\n",
    "\n",
    "    # print(len(interactions[]))\n",
    "    base_model = AlternatingLeastSquares(\n",
    "        factors=5, \n",
    "        iterations=75, \n",
    "        regularization=0.05, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # interactions.eliminate_zeros()\n",
    "    # print(interactions.T)\n",
    "\n",
    "    base_model.fit(interactions)\n",
    "    \n",
    "    # orgs_to_filter = list(np.arange(inner_orgs_len))\n",
    "\n",
    "    # recommendations = base_model.recommend_all(\n",
    "        # interactions,\n",
    "        # N=N,\n",
    "        # filter_already_liked_items=True,\n",
    "        # filter_items=orgs_to_filter,\n",
    "        # show_progress=True\n",
    "    # )\n",
    "    \n",
    "    # for user_id in tqdm(X_test['user_id'].values, leave=False):\n",
    "        # predictions.loc[user_id, 'target'] = list(\n",
    "            # map(\n",
    "                # lambda org_idx: idx_to_oid[org_idx], \n",
    "                # recommendations[uid_to_idx[user_id]]\n",
    "            # )\n",
    "        # )\n",
    "        \n",
    "    # return predictions.reset_index()\n",
    "\n",
    "msk_predictions = make_predictions(msk_interactions, test_msk_users, N)\n",
    "# spb_predictions = make_predictions(spb_interactions, test_spb_users, N)\n",
    "\n",
    "predictions = pd.concat([msk_predictions, spb_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1799a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print_score(mapN(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f170261",
   "metadata": {},
   "source": [
    "Источники\n",
    "1. **Ким Фалк** - Практические рекомендательные системы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38768da3",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Выберем лучший метод на валидации, переобучим его на всей выборке и сделаем предсказание на тестовой выборке. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e9dfc",
   "metadata": {},
   "source": [
    "## Without ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = pd.read_csv('data/test_users.csv', sep=',')\n",
    "test_users['city'] = test_users.merge(users, on='user_id')['city']\n",
    "\n",
    "tourist_reviews = reviews[(reviews['ts'] > 900) & (reviews['user_city'] != reviews['org_city'])]\n",
    "\n",
    "msk_orgs = extract_top_by_rubrics(tourist_reviews[tourist_reviews['org_city'] == 'msk'], N)\n",
    "spb_orgs = extract_top_by_rubrics(tourist_reviews[tourist_reviews['org_city'] == 'spb'], N)\n",
    "\n",
    "msk_orgs = str(' '.join(map(str, msk_orgs)))\n",
    "spb_orgs = str(' '.join(map(str, spb_orgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "choose = lambda x: spb_orgs if x['user_id'] == 'msk' else msk_orgs\n",
    "target = test_users.apply(choose, axis=1)\n",
    "\n",
    "predictions = test_users[['user_id']]\n",
    "predictions['target'] = target\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('answers.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e9ac5",
   "metadata": {},
   "source": [
    "## With ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208dc926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_users = pd.read_csv('data/test_users.csv', sep=',')\n",
    "test_users = test_users.merge(users, on='user_id')\n",
    "\n",
    "\n",
    "test_msk_users = test_users[test_users['city'] == 'msk'][['user_id', 'city']]\n",
    "test_spb_users = test_users[test_users['city'] == 'spb'][['user_id', 'city']]\n",
    "\n",
    "\n",
    "msk_interactions = interaction_matrix(\n",
    "    reviews[reviews['user_city'] == 'msk'],\n",
    "    test_msk_users\n",
    ")\n",
    "\n",
    "spb_interactions = interaction_matrix(\n",
    "    reviews[reviews['user_city'] == 'spb'],\n",
    "    test_spb_users\n",
    ")\n",
    "\n",
    "msk_predictions = make_predictions(msk_interactions, test_msk_users, N)\n",
    "spb_predictions = make_predictions(spb_interactions, test_spb_users, N)\n",
    "\n",
    "predictions = pd.concat([msk_predictions, spb_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e67b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['target'] = predictions['target'].apply(lambda orgs: ' '.join(map(str, orgs)))\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68205483",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('answers_ml.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db552b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eea2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df4322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bac21b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50193711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f16c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.github.com/events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "events = json.loads(r.text)"
   ]
  }
 ],
 "metadata": {
  "jupytercloud": {
   "vault": {
    "secrets": [
     {
      "name": "yt_secrets",
      "uuid": "sec-01fd5qjpw0xwwpv6t83wc41tp8"
     }
    ]
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
